#include "Rohan.h"
#include "Rohan-data.h"
#include "Rohan-io.h"
#include "Rohan-learn.h"
#include "Rohan-menu.h"
#include "Rohan-kernel.h"
#include "Rohan-class.h"
#include "ShowMe.h"
#include "stdafx.h"
#include <conio.h> //for _getch 

#include <cuda.h>
#include <cublas.h>
#include <cutil_inline.h>
#include <cuda_runtime_api.h>
#include <multithreading.h>

//#include <time.h> // for tsrtuct
#include <sys/timeb.h>
#include <iostream>
#include <stdlib.h>
using namespace std;
using std::cin;
using std::cout;

#define TWO_PI 6.283185307179586476925286766558
#define IDX2C(i,j,ld) (((j)*(ld))+(i))

extern int iDebugLvl, iWarnings, iErrors, iTrace;
extern bool bCublasAvailable;


//////////////// class cBarge begins ////////////////

void cBarge::ShowMe()
{
	//ShowMeSes(* rSes, false);
	printf("Am stout barje.\n");
}

long cBarge::SetContext( rohanContext& rC)
{/// enables pointer access to master context struct
	rSes = &rC;
	return 0;
}

long cBarge::SetDrover( class cDrover * cdDrover)
{/// enables pointer access to active Drover object
	Drover = cdDrover;
	return 0;
}
long cBarge::SetTeam( class cTeam * ctTeam)
{/// enables pointer access to active Team object
	Team = ctTeam;
	return 0;
}


long cBarge::ObtainSampleSet(struct rohanContext& rSes)
{mIDfunc /// chooses and loads the learning set to be worked with Ante-Loop
	int iReturn=0; 
	//rSes.rLearn->bContInputs=false;
	//rSes.rLearn->iContOutputs=(int)false;
	cout << "Samples treated as discrete or continuous by fractionality. XX" << endl;

	printf("Enter 0 for 10K-set, weights\n\t 1 for 3-set, weights\n\t 2 for 150-set, no wgts\n\t 3 for 3-set, no wgts\n\t 4 for 2-1 rand weights");
	printf("\n\t 5 for 416 samples x 200 inputs\nEnter 10+ for basic diag\n\t30+ for more diag\n\t70+ for full diag\n");
	std::cin >> iDebugLvl;
	//cout << "Loading trivial3 test learning set" << endl;
	//iDebugLvl=4;
	switch ( iDebugLvl % 10) {
		case 0:
		  rSes.rLearn->sLearnSet="AirplanePsDN1W3S10k.txt";
		  break;
		case 1:
		  rSes.rLearn->sLearnSet="trivial.txt";
		  break;
		case 2:
		  rSes.rLearn->sLearnSet="iris.txt";
		  break;
		case 3:
		  rSes.rLearn->sLearnSet="trivial2.txt";
		  break;
		case 4:
		  rSes.rLearn->sLearnSet="trivial3.txt";	
		  break;
		case 5:
		  rSes.rLearn->sLearnSet="PC-63-32-200-LearnSet.txt";
		  break;
		default:
		  rSes.rLearn->sLearnSet="iris.txt";
		  break;
	}
	rSes.iDebugLvl=iDebugLvl/=10; // drop final digit
	fprintf(stderr, "Debug level is %d.\n", iDebugLvl);
	FILE *fileInput;
	// File handle for input
	
	iReturn=AsciiFileHandleRead(rSes.rLearn->sLearnSet, &fileInput);
	if (iReturn==0) // unable to open file
		++rSes.iErrors;
	else{ // file opened normally
		// file opening and reading are separated to allow for streams to be added later
		long lLinesRead=cuLoadSampleSet(rSes, fileInput);
		if (lLinesRead) {
			printf("Parsed %d lines from %s\nStored %d samples, %d input values, %d output values each.\n", 
				lLinesRead, rSes.rLearn->sLearnSet, rSes.rLearn->lSampleQty, rSes.rLearn->iInputQty, rSes.rLearn->iOutputQty);
				//load samples into the parallel structures in the GPU memory
			if(CurateSectorValue(rSes)) {
				if(rSes.bCublasAvailable){
					devCopySampleSet(rSes);
					//cout << endl << "<" << rSes.rLearn->rSample[0].cdcDOutputs[0].y << "&"; cout << rSes.rLearn->rSample[0].cdcAltDOutputs[0].y << ">" << endl;
				}
			}
			else{
				return 0;
			} //endif for CurateSectorValue
		}
		else {
			printf("No Samples Read by cuLoadSampleSet\n");
			iReturn=0;
		}
	}
	return iReturn;
}

long cBarge::DoCuMakeLayers(int iInputQty, char *sLayerSizes, struct rohanContext& rSes)
{mIDfunc
/// Parses a string to assign network architecture parameters for use by later functions. 
/// Returns neurons in last layer if successful, otherwise 0
	char *sArchDup, *sDummy;
	int iLayerQty=1;

	sArchDup = _strdup(sLayerSizes); // strtok chops up the input string, so we must make a copy (or do we? - 6/15/10) (yes we do 8/22/10)
	sDummy = strtok (sArchDup, " ,\t");
	while (sDummy!=NULL) {// this loop counts the values present in a copy of sLayerSizes, representing neurons in each layer until a not-legal layer value is reached
		sDummy = strtok (NULL, " ,\t");
		++iLayerQty; //count layers
		//printf("%d-%s\n",iLayerQty, sDummy);
	}
	rSes.rNet->rLayer=(struct rohanLayer*)malloc(iLayerQty * sizeof (struct rohanLayer)); //point to array of layers
		mCheckMallocWorked(rSes.rNet->rLayer)
	printf("%d layers plus input layer allocated.\n", (iLayerQty-1));
	
	sArchDup=_strdup(sLayerSizes); // second pass
	sDummy = strtok(sArchDup, " ,\t");
	for (int i=0;i<iLayerQty;++i) {// this loop stores neurons in each layer
		//printf("%d-%s/%d\n",i, sDummy, atoi(sDummy));
		//printf ("%s Layer %d neurons %d\n",sDummy, i, rSes.rNet->rLayer[i].iNeuronQty);
		if (i) {
			rSes.rNet->rLayer[i].iNeuronQty = atoi(sDummy);
			rSes.rNet->rLayer[i].iDendriteQty=rSes.rNet->rLayer[i-1].iNeuronQty; //previous layer's neuron qty is dendrite qty
			sDummy = strtok (NULL, " ,\t");
		}
		else {
			rSes.rNet->rLayer[i].iNeuronQty = iInputQty; // layer zero has virtual neurons with outputs equal to inputs converted to phases
			rSes.rNet->rLayer[0].iDendriteQty=0; // layer zero has no dendrites
		}
		printf ("Layer %d: %d nodes\n", i, rSes.rNet->rLayer[i].iNeuronQty);
	}
	if (DoCuMakeNNStructures(rSes)) 
		printf("Nodes allocated.");
	return rSes.rNet->rLayer[rSes.rNet->iLayerQty-1].iNeuronQty;
}


long cBarge::ObtainNNTop(struct rohanContext& rSes)
{mIDfunc /// sets up network poperties and data structures for use
	char sNeuronsPerLayer[254];
	int iSectorQty, iInputQty;

	cout << "Enter # of sectors (0 to return): ";
	cin >> iSectorQty;
	if(iSectorQty){
		cout << "Enter # of inputs (0 to return): ";
		cin >> iInputQty; // last chance to quit
	}
	if(iSectorQty && iInputQty) {
		DoCuFreeNNTop(rSes); // release old network structures
		rSes.rNet->iSectorQty=iSectorQty; // update sector qty
		rSes.rLearn->iInputQty=iInputQty; // upsdate input qty
		cout << "Enter numbers of neurons per layer separated by commas, \ne.g. 63,18,1 : ";
		cin >> sNeuronsPerLayer;
		DoCuMakeLayers(iInputQty, sNeuronsPerLayer, rSes); // make new layers
		rSes.rNet->dK_DIV_TWO_PI = rSes.rNet->iSectorQty / TWO_PI; // Prevents redundant conversion operations
		DoCuMakeNNStructures(rSes); // allocates memory and populates network structural arrays
		DoCuRandomizeWeights(rSes); // populate newtork with random weight values
		printf("Random weights loaded.\n");
		printf("%d-valued logic sector table made.\n", DoCuSectorTableMake(rSes));
		printf("\n");
		return rSes.rNet->iLayerQty;
	}
	else
		return 999;
}

long cBarge::DoCuPrepareNetwork(struct rohanContext& rSes)
{mIDfunc /// sets up network properties and data structures for use in host memory space
	int iReturn;
	DoCuMakeNNStructures(rSes); // allocates memory and populates network structural arrays
	iReturn=BinaryFileHandleRead(rSes.rNet->sWeightSet, &rSes.rNet->fileInput);
	// file opening and reading are separated to allow for streams to be added later
	if (iReturn) {
		long lWeightsRead=cuNNLoadWeights(rSes, rSes.rNet->fileInput);
			if (lWeightsRead) printf("Parsed and assigned %d complex weights from %s\n", lWeightsRead, rSes.rNet->sWeightSet);
			else {
				fprintf(stderr, "Error: No Weights Read by cuNNLoadWeights\n");
				++rSes.iErrors;
				printf("Waiting on keystroke...\n"); _getch(); return iReturn;
			}
	}
	else { // can't open, user random weights
		printf("Can't open %s, using random weights.\n", rSes.rNet->sWeightSet);
		DoCuRandomizeWeights(rSes); // populate network with random weight values
	}
	iReturn=DoCuSectorTableMake(rSes); // fill the table with values
	if (iReturn==0) {
		printf("Out of Memory in cuSectorTableMake\n");
		++rSes.iErrors;
		printf("Waiting on keystroke...\n");
		_getch();
	}
	return iReturn;
}

long cBarge::DoPrepareNetwork(struct rohanContext& rSes)
{mIDfunc /// sets up network poperties and data structures for use
	int iReturn=0;
	// on with it
	//rSes.rNet->dK_DIV_TWO_PI = rSes.rNet->iSectorQty / TWO_PI; // Prevents redundant conversion operations
	iReturn=DoCuPrepareNetwork(rSes);
	if (rSes.bCublasAvailable){
		devPrepareNetwork(rSes);
	}
	return iReturn;
}


long cBarge::DoCuMakeNNStructures(struct rohanContext &rSes)
{mIDfunc
/*! Initializes a neural network structure of the given number of layers and
 *  layer populations, allocates memory, and populates the set of weight values randomly.
 *
 * iLayerQty = 3 means Layer 1 and Layer 2 are "full" neurons, with output-only neurons on layer 0.
 * 0th neuron on each layer is a stub with no inputs and output is alawys 1+0i, to accomodate internal weights of next layer.
 * This allows values to be efficiently calculated by referring to all layers and neurons identically.
 * 
 * iNeuronQty[1] is # of neurons in Layer 1
 * iNeuronQty[2] is # of neurons in Layer 2
 * iNeuronQty[0] is # of inputs in Layer 0 */
	//int iUnderLayer; 
	long lReturn=0;
	cublasStatus csStatus;
	cuDoubleComplex cdcInit;
	// complex weights
	for (int i=1; i < rSes.rNet->iLayerQty; ++i){  //Layer Zero has no need of weights! 8/13/2010
		struct rohanLayer& lay = rSes.rNet->rLayer[i];
		//allocate a pointer to an array of weights (2d unrolled to 1d)
		lay.cdcWeights=(cuDoubleComplex*)malloc((lay.iNeuronQty+1) * (lay.iDendriteQty+1) * sizeof(cuDoubleComplex));
			mCheckMallocWorked(lay.cdcWeights)
		/*lay.cdcWeights=(cuDoubleComplex**)malloc((lay.iNeuronQty+1) * sizeof(cuDoubleComplex*));
			mCheckMallocWorked(lay.cdcWeights)*/
		lReturn+=lay.iNeuronQty*lay.iDendriteQty;
   		for (int j=0; j <= lay.iNeuronQty; ++j){ 
			//lay.cdcWeights[j] = (cuDoubleComplex*)malloc((lay.iDendriteQty+1) * sizeof (cuDoubleComplex)); //allocate a pointer to an array of weights
			//	mCheckMallocWorked(lay.cdcWeights[j])
			for (int k=0; k <= lay.iDendriteQty; ++k){
				lay.cdcWeights[IDX2C(j, k, lay.iNeuronQty+1)].x=(double)rand()/65535; // necessary to promote one operand to double to get a double result
				lay.cdcWeights[IDX2C(j, k, lay.iNeuronQty+1)].y=(double)rand()/65535; // necessary to promote one operand to double to get a double result
			}
		}
		// reset neuron 0 weights to null
		for (int k=0; k <= lay.iDendriteQty; ++k){
			lay.cdcWeights[IDX2C(0, k, lay.iNeuronQty+1)].x=0;
			lay.cdcWeights[IDX2C(0, k, lay.iNeuronQty+1)].y=0;
		}
		lay.cdcWeights[IDX2C(0, 0, lay.iNeuronQty+1)].x=1; // neuron 0 interior weight should always be equal to 1

		// allocation loops repeated separately in hopes of eliminating a memory frag problem causing cublasGetMatrix to cdcAltWeights to mangle both 5/23/11
		// This was successful!!! 5.23.11
		//lay.cdcAltWeights=(cuDoubleComplex**)malloc((lay.iNeuronQty+1) * sizeof(cuDoubleComplex*));  //allocate a pointer to an array of weights for comparison purposes
		//	mCheckMallocWorked(lay.cdcAltWeights)
		//for (int j=0; j <= lay.iNeuronQty; ++j){ 
		//	lay.cdcAltWeights[j] = (cuDoubleComplex*)malloc((lay.iDendriteQty+1) * sizeof (cuDoubleComplex)); //allocate a pointer to an array of weights for comparison purposes
		//		mCheckMallocWorked(lay.cdcAltWeights[j])
		//}
		lay.cdcAltWeights=(cuDoubleComplex*)malloc((lay.iNeuronQty+1) * (lay.iDendriteQty+1) * sizeof(cuDoubleComplex));  //allocate a pointer to an array of weights for comparison purposes
			mCheckMallocWorked(lay.cdcAltWeights)
		//allocate a GPU-space pointer to a matrix of neurons x weights for each layer
		csStatus=cublasAlloc((lay.iNeuronQty+1)*(lay.iDendriteQty+1), sizeof(cuDoubleComplex), (void**)&lay.gpuWeights);
			mCuMsg(csStatus,"cublasAlloc()")
		//allocate a GPU-space pointer to an inverted matrix of reciprocal values for each layer
		csStatus=cublasAlloc((lay.iNeuronQty+1)*(lay.iDendriteQty+1), sizeof(cuDoubleComplex), (void**)&lay.gpuUnWeights);
			mCuMsg(csStatus,"cublasAlloc()")
	}
	// complex outputs
	cdcInit.x=-999; cdcInit.y=999;
	for (int i=0; i < rSes.rNet->iLayerQty; ++i){
			struct rohanLayer& lay = rSes.rNet->rLayer[i];
		lay.cdcZOutputs = (cuDoubleComplex*)malloc((lay.iNeuronQty+1) * sizeof (cuDoubleComplex)); //allocate a pointer to an array of outputs
			mCheckMallocWorked(lay.cdcZOutputs)
		lay.cdcZAltOutputs = (cuDoubleComplex*)malloc((lay.iNeuronQty+1) * sizeof (cuDoubleComplex)); //allocate a pointer to an alternate array of outputs for comparison
			mCheckMallocWorked(lay.cdcZAltOutputs)
		lay.cdcDeltas = (cuDoubleComplex*)malloc((lay.iNeuronQty+1) * sizeof (cuDoubleComplex)); //allocate a pointer to an parallel array of learned corrections
			mCheckMallocWorked(lay.cdcDeltas)
		lay.cdcAltDeltas = (cuDoubleComplex*)malloc((lay.iNeuronQty+1) * sizeof (cuDoubleComplex)); //allocate a pointer to an alternate array of learned corrections
			mCheckMallocWorked(lay.cdcAltDeltas)
		lReturn+=lay.iNeuronQty;
		for (int j=0; j <= lay.iNeuronQty; ++j){
			lay.cdcZOutputs[j]=lay.cdcZAltOutputs[j]=lay.cdcDeltas[j]=lay.cdcAltDeltas[j]=cdcInit;
		}
		//allocate a GPU-space pointer to a vector of complex neuron outputs for each layer
		csStatus=cublasAlloc(lay.iNeuronQty+1, sizeof(cuDoubleComplex), (void**)&lay.gpuZOutputs); 
			mCuMsg(csStatus,"cublasAlloc()")
	}
	return lReturn; //return how many weights and outputs allocated
}


long cBarge::DoCuSectorTableMake(struct rohanContext &rSes)
{mIDfunc /// allocate and populate an array of complex coordinates for sectors on the unit circle of the complex plane
	double two_pi_div_sect_qty = TWO_PI/rSes.rNet->iSectorQty;

	rSes.rNet->cdcSectorBdry=(cuDoubleComplex*)malloc(rSes.rNet->iSectorQty * sizeof (cuDoubleComplex)); //point to array of cdc's
		mCheckMallocWorked(rSes.rNet->cdcSectorBdry)
	rSes.rNet->cdcAltSectorBdry=(cuDoubleComplex*)malloc(rSes.rNet->iSectorQty * sizeof (cuDoubleComplex)); //point to array of cdc's
		mCheckMallocWorked(rSes.rNet->cdcAltSectorBdry)
	for (int s=0; s<rSes.rNet->iSectorQty; ++s) {
		rSes.rNet->cdcSectorBdry[s].x=cos(s*two_pi_div_sect_qty);
		rSes.rNet->cdcSectorBdry[s].y=sin(s*two_pi_div_sect_qty);
		rSes.rNet->cdcAltSectorBdry[s]=cdcIdentity;
	}
	return rSes.rNet->iSectorQty;
}

long cBarge::DoCuRandomizeWeights(struct rohanContext &rSes)
{mIDfunc /// generates random weights in [0..1]
	long lReturnValue=0;

	for (int i=1; i < rSes.rNet->iLayerQty; ++i){ //no weights for layer 0
		struct rohanLayer& lay = rSes.rNet->rLayer[i];
		// set neuron 0 's weights to nil
		for (int k=1; k <= lay.iDendriteQty; ++k){
			lay.cdcWeights[IDX2C(0, k, lay.iNeuronQty+1)].x=0;
			lay.cdcWeights[IDX2C(0, k, lay.iNeuronQty+1)].y=0;
		}
		lay.cdcWeights[IDX2C(0, 0, lay.iNeuronQty+1)].x=1; // NZero interior weight should always be equal to 1+0i
		lay.cdcWeights[IDX2C(0, 0, lay.iNeuronQty+1)].y=0;
		for (int j=1; j <= lay.iNeuronQty; ++j){ // weights for neurons 1+
			for (int k=0; k <= lay.iDendriteQty; ++k){
				cuDoubleComplex& way = lay.cdcWeights[IDX2C(j, k, lay.iNeuronQty+1)];
				way.x=(double)rand()/RAND_MAX;
				way.y=(double)rand()/RAND_MAX;
				++lReturnValue;
			}
		}
	}
	printf("%d random weights on [0..1]\n",lReturnValue);
	cuResetAllDeltasAndOutputs(rSes);
	return lReturnValue;
}
long cBarge::DoCuFreeNNTop(struct rohanContext &rSes)
{mIDfunc/// frees data structures related to network topology
	cublasStatus csStatus;
	
	free( rSes.rNet->cdcSectorBdry );
	// layer components
	free( rSes.rNet->rLayer[0].cdcZOutputs ); // Layer Zero has no need of weights!
	csStatus = cublasFree( rSes.rNet->rLayer[0].gpuZOutputs ); // de-allocate a GPU-space pointer to a vector of complex neuron outputs for each layer
	mCuMsg(csStatus,"cublasFree()")
	
	for (int i=1; i < rSes.rNet->iLayerQty; ++i){ 
		struct rohanLayer& lay=rSes.rNet->rLayer[i];
		//for (int j=0; j <= lay.iNeuronQty; ++j){ // Virtual Neuron Zero preceeds adjustable weight neurons
			//free( lay.cdcWeights[j] ); // de-allocate a pointer to an array of weights
			//free( lay.cdcAltWeights[j] ); // de-allocate a pointer to an array of weights
		//}
		free( lay.cdcWeights ); // de-allocate a pointer to an array of arrays of weights
		free( lay.cdcAltWeights ); // de-allocate a pointer to an array of arrays of weights
		free( lay.cdcDeltas ); // free the backprop areas
		//free( lay.cdcZOutputs ); // free the outputs
		//free( lay.cdcLinearWeights );
		csStatus = cublasFree( lay.gpuWeights ); // de-allocate a GPU-space pointer to a matrix of neurons x weights for each layer
		mCuMsg(csStatus,"cublasFree()");
		csStatus = cublasFree( lay.gpuZOutputs ); // de-allocate a GPU-space pointer to a vector of complex neuron outputs for each layer
		mCuMsg(csStatus,"cublasFree()");
		csStatus = cublasFree( lay.gpuDeltas);
		mCuMsg(csStatus,"cublasFree()");
		csStatus = cublasFree( lay.gpuUnWeights);
		mCuMsg(csStatus,"cublasFree()");
	}
	free( rSes.rNet->rLayer ); // free empty layers
	printf("Network structures freed.\n");
	return 0;
}


long cBarge::DoCuFreeLearnSet(struct rohanContext &rSes)
{mIDfunc/// free the learning set of samples
	
	for (long s=0; s<rSes.rLearn->lSampleQty; ++s){ // iterate over the number of samples
		struct rohanSample& sam = rSes.rLearn->rSample[s]; 
		free( sam.dXInputs ); free( sam.dTOutputs );
		free( sam.dYEval ); free( sam.dAltYEval ); free( sam.cdcYEval ); free( sam.cdcAltYEval );
		free( sam.cdcXInputs ); free( sam.cdcAltXInputs );
		free( sam.cdcDOutputs ); free( sam.cdcAltDOutputs );
		cublasFree( sam.gpuXInputs );
		cublasFree( sam.gpuDOutputs ); 
		cublasFree( sam.gpudYEval ); 
		cublasFree( sam.gpuYEval ); 
	}
	free( rSes.rLearn->rSample );
	cublasFree( rSes.rLearn->gpudSE1024 );
	printf("Learning set structures freed.\n");

	return 0;
}

long cBarge::DoCuFree(struct rohanContext &rSes)
{mIDfunc/// free allocated memory for all structures
	
	DoCuFreeNNTop(rSes); // free network topology structures
	DoCuFreeLearnSet(rSes); // free learning set structures
	
	return 0;
}


long cBarge::DoDevCopyNNStructures(struct rohanContext &rSes)
{mIDfunc
/*! Initializes a parallel neural network structure in GPU-memory with the same given number of layers and
 *  layer populations, allocates memory, and populates the set of weight values randomly.
 *
 * iLayerQty = 3 means Layer 1 and Layer 2 are "full" neurons, with output-only neurons on layer 0.
 * 0th neuron on each layer is a stub with no inputs and output is alawys 1+0i, to accomodate internal weights of next layer.
 * This allows values to be efficiently calculated by referring to all layers and neurons identically.
 * 
 * iNeuronQty[1] is # of neurons in Layer 1
 * iNeuronQty[2] is # of neurons in Layer 2
 * iNeuronQty[0] is # of inputs in Layer 0 */
	//int iUnderLayer; 
	long lReturn=0;
	cublasStatus csStatus;
	
	// complex weights
	for (int i=1; i < rSes.rNet->iLayerQty; ++i){  //Layer Zero has no need of weights! 8/13/2010
		struct rohanLayer& lay = rSes.rNet->rLayer[i];
		//allocate a GPU-space pointer to a matrix of neurons x weights for each layer
		csStatus=cublasAlloc((lay.iNeuronQty+1)*(lay.iDendriteQty+1), sizeof(cuDoubleComplex), (void**)&lay.gpuWeights);
			mCuMsg(csStatus,"cublasAlloc()")
		//allocate a GPU-space pointer to a matrix of reversed neuron weights for each layer
		csStatus=cublasAlloc((lay.iNeuronQty+1)*(lay.iDendriteQty+1), sizeof(cuDoubleComplex), (void**)&lay.gpuUnWeights);
			mCuMsg(csStatus,"cublasAlloc()")	
	}
	// complex outputs
	for (int i=0; i < rSes.rNet->iLayerQty; ++i){
			struct rohanLayer& lay = rSes.rNet->rLayer[i];
		lReturn+=lay.iNeuronQty;
		//allocate a GPU-space pointer to a vector of complex neuron outputs for each layer
		csStatus=cublasAlloc(lay.iNeuronQty+1, sizeof(cuDoubleComplex), (void**)&lay.gpuZOutputs); 
			mCuMsg(csStatus,"cublasAlloc()")
		//allocate a GPU-space pointer to a vector of complex error corrections for each layer
		csStatus=cublasAlloc(lay.iNeuronQty+1, sizeof(cuDoubleComplex), (void**)&lay.gpuDeltas); 
			mCuMsg(csStatus,"cublasAlloc()")
	}
	return lReturn; //return how many weights and outputs allocated
}


long cBarge::DoDevCopySectorTable(struct rohanContext &rSes)
{mIDfunc /// allocate and populate an array of complex coordinates for sectors on the unit circle of the complex plane
	cublasStatus csStatus;

	csStatus=cublasAlloc(rSes.rNet->iSectorQty, sizeof(cuDoubleComplex), (void**)&(rSes.rNet->gpuSectorBdry));
			mCuMsg(csStatus,"cublasAlloc()")
	csStatus=cublasSetVector(rSes.rNet->iSectorQty, sizeof(cuDoubleComplex), rSes.rNet->cdcSectorBdry, 1, rSes.rNet->gpuSectorBdry, 1);
		mCuMsg(csStatus,"cublasSetVector()") // and copies back to CPU again for comparison
	return rSes.rNet->iSectorQty;
}


long cBarge::DoDualRandomizeWeights(struct rohanContext &rSes)
{mIDfunc /// generates random weights in [0..1]
	cublasStatus csStatus;
	long lReturnValue=0;

	for (int i=1; i < rSes.rNet->iLayerQty; ++i){ //no weights for layer 0
		struct rohanLayer& lay = rSes.rNet->rLayer[i];
		// set neuron 0 's weights to nil
		for (int k=1; k <= lay.iDendriteQty; ++k){
			lay.cdcWeights[IDX2C(0, k, lay.iNeuronQty+1)].x=0;
			lay.cdcWeights[IDX2C(0, k, lay.iNeuronQty+1)].y=0;
		}
		lay.cdcWeights[IDX2C(0, 0, lay.iNeuronQty+1)].x=1; // NZero interior weight should always be equal to 1+0i
		lay.cdcWeights[IDX2C(0, 0, lay.iNeuronQty+1)].y=0;
		for (int j=1; j <= lay.iNeuronQty; ++j){ // weights for neurons 1+
			for (int k=0; k <= lay.iDendriteQty; ++k){
				cuDoubleComplex& way = lay.cdcWeights[IDX2C(j, k, lay.iNeuronQty+1)];
				way.x=(double)rand()/RAND_MAX;
				way.y=(double)rand()/RAND_MAX;
				++lReturnValue;
			}
		}
		csStatus=cublasSetMatrix(lay.iNeuronQty+1, lay.iDendriteQty+1, sizeof(cuDoubleComplex),
			lay.cdcWeights, lay.iNeuronQty+1, lay.gpuWeights, lay.iNeuronQty+1);
			mCuMsg(csStatus,"cublasSetMatrix()")
		
	}
	printf("%d random weights on [0..1] generated and transferred.\n",lReturnValue);
	cuResetAllDeltasAndOutputs(rSes);
	devResetAllDeltasAndOutputs(rSes);
	return lReturnValue;
}

long cBarge::DoDevPrepareNetwork(struct rohanContext& rSes)
{mIDfunc /// sets up network properties and data structures for use in GPU memory space
	int iReturn;
	iReturn=DoDevCopyNNStructures(rSes);
	iReturn+=devCopyNNWeights(rSes);
	iReturn+=DoDevCopySectorTable(rSes);
	cout << iReturn << " network values transfered to GPU memory space." << endl;
	return iReturn;
}
